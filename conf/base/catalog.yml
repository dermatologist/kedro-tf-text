# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/data/data_catalog.html

## ID | Long line of text
text_data:
  type: pandas.CSVDataSet
  filepath: data/01_raw/test_report.csv

## ID | included | fields | excluded | fields | outcome (y)
tabular_data:
  type: pandas.CSVDataSet
  filepath: data/01_raw/test_dataset.csv


# pretrained_embedding:
#   type: text.TextDataSet
#   filepath: data/04_feature/new_data_embed300.txt

# vocab_json:
#   type: json.JSONDataSet
#   filepath: data/04_feature/vocab.json

word2vec_embedding:
  type: pickle.PickleDataSet
  filepath: data/06_models/word2vec-embedding.pkl

glove_embedding:
  type: pickle.PickleDataSet
  filepath: data/06_models/glove-embedding.pkl

tabular_model:
  type: pickle.PickleDataSet
  filepath: data/06_models/tabular_model.pkl

processed_text:
  type: pickle.PickleDataSet
  filepath: data/03_primary/processed-text.pkl

image_data:
  type: PartitionedDataSet
  dataset: pillow.ImageDataSet
  path: data/01_raw/imageset
  filename_suffix: ".jpg"

chexnet_model:
  type: tensorflow.TensorFlowModelDataset
  filepath: data/06_models/densenet-chexnet


fusion_model:
  type: tensorflow.TensorFlowModelDataset
  filepath: data/07_model_output/fusion

datasetinmemory:
  type: MemoryDataSet
  copy_mode: assign

bert_model:
  type: kedro_tf_text.extras.datasets.bert_model_download.BertModelDownload
  preprocessor_url: "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"
  encoder_url: "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"

bert_model_saved:
  type: tensorflow.TensorFlowModelDataset
  filepath: data/06_models/bert-tf